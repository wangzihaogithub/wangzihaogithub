---
layout: post
title:  "大模型-agent系列论文"
tags: 大模型
---

参考来源：
 
 [OpenAI科学家LilianWeng的博客](https://lilianweng.github.io/ "https://lilianweng.github.io/")

 [Agent智能代理（来自OpenAI科学家LilianWeng)](https://lilianweng.github.io/posts/2023-06-23-agent/ "https://lilianweng.github.io/posts/2023-06-23-agent/")

 [Tree of Thoughts思维树2023(论文)](https://arxiv.org/abs/2305.10601 "https://arxiv.org/abs/2305.10601")

 [CoT思维链2022(论文)](https://arxiv.org/abs/2201.11903 "https://arxiv.org/abs/2201.11903")  

 [ReAct自我反思-Thought: ... Action: ... Observation: ..2022(论文)](https://arxiv.org/abs/2210.03629 "https://arxiv.org/abs/2210.03629")

 [Reflexion动态记忆和自我反思2023(论文)](https://arxiv.org/abs/2303.11366 "https://arxiv.org/abs/2303.11366")

 [Chain of Hindsight后置链2023(论文)](https://arxiv.org/abs/2302.02676 "https://arxiv.org/abs/2302.02676")



### 2022年1月28  论文：《大语言模型中的思维链提示引发推理》

[CoT思维链2022(论文)](https://arxiv.org/abs/2201.11903 "https://arxiv.org/abs/2201.11903")

摘要：我们探讨了生成思维链（一系列中间推理步骤）如何显著提高大型语言模型执行复杂推理的能力。

特别是，我们通过一种称为思维链提示的简单方法展示了这种推理能力是如何在足够大的语言模型中自然出现的，

其中提供了一些思维链演示作为提示的示例。

对三个大型语言模型的实验表明，思维链提示可以提高一系列算术、常识和符号推理任务的性能。实证收益可能是惊人的。

例如，仅用 8 个思维链示例提示一个 540B 参数的语言模型，就可以在 GSM8K 数学单词问题基准测试中达到最先进的准确性，

甚至超过了使用验证器微调的 GPT-3。


### 2023年5月17 论文：《思维之树：利用大型语言模型进行深思熟虑的问题解决》

[Tree of Thoughts思维树2023(论文)](https://arxiv.org/abs/2305.10601 "https://arxiv.org/abs/2305.10601")

摘要：语言模型越来越多地被部署用于各种任务的一般问题解决，但在推理过程中仍然局限于符号级别的从左到右的决策过程。

这意味着在需要探索、策略性前瞻或初始决策起关键作用的任务中，它们可能会表现不足。

为了克服这些挑战，我们引入了一种新的语言模型推理框架 —— 思维树（ToT），

它对流行的思维链提示语言模型的方法进行了泛化，并能够在连贯的文本单元（思想）上进行探索，

这些单元作为解决问题的中间步骤。ToT 允许语言模型通过考虑多种不同的推理路径和自我评估选择来决定下一步行动，

以及在必要时进行前瞻或回溯以做出全局选择，从而进行深思熟虑的决策。

我们的实验表明，ToT 在三个需要非平凡规划或搜索的新任务（24 点游戏、创意写作和迷你填字游戏）上显著增强了语言模型的问题解决能力。

例如，在 24 点游戏中，虽然带有思维链提示的 GPT-4 仅解决了 4% 的任务，

但我们的方法成功率达到了 74%。包含所有提示的代码库：此 https URL https://github.com/princeton-nlp/tree-of-thought-llm。

      原提示词写法
      standard_prompt=''
      写一篇连贯的四段短文。每段的结束句必须是：{input}
      '''

      cot_prompt=''
      写一篇连贯的四段短文。每段的结束句必须是：{input}
      制定一个计划，然后写下来。您的输出应采用以下格式：
      计划：
      你的计划在这里。
      文章
      你在这里的通道。
      '''

      vote_frompt=''给出一条指令和几个选项，决定哪个选项最有希望。详细分析每个选项，然后在最后一行得出结论“最佳选择是{s}”，其中s是选项的整数id。
      '''

      compare_frompt=''简要分析以下两段的连贯性。在最后一行结束“连贯性更强的段落是1”，“连贯性更好的段落是2”，或“这两个段落连贯性相似”。
      '''

      score_prompt=''分析以下段落，然后在最后一行得出结论“因此，连贯性得分为{s}”，其中s是1到10的整数。
      '''


### 2022年10月6  论文：《大语言模型中的思维链提示引发推理》

[ReAct自我反思-Thought: ... Action: ... Observation: ..2022(论文)](https://arxiv.org/abs/2210.03629 "https://arxiv.org/abs/2210.03629")

语言模型在推理（例如思维链提示）和行动（例如 WebGPT、SayCan、ACT-1）方面越来越好，但这两个方向仍然是分开的。

ReAct：如果将这两项基本功能结合起来会怎样？

摘要：虽然大型语言模型 （LLM） 在语言理解和交互式决策方面的任务中表现出令人印象深刻的能力，

但它们的推理能力（例如思维链提示）和行动能力（例如行动计划生成）主要作为单独的主题进行研究。

在本文中，我们探讨了如何使用 LLM 以交错方式生成推理跟踪和特定于任务的操作，

从而在两者之间实现更大的协同作用：推理跟踪帮助模型诱导、跟踪和更新行动计划以及处理异常，

而操作允许它与外部来源（例如知识库或环境）交互。 

以收集其他信息。我们将名为 ReAct 的方法应用于各种语言和决策任务，并在最先进的基线上证明其有效性，

以及与没有推理或操作组件的方法相比，提高了人类的可解释性和可信度。

具体来说，在问答 （HotpotQA） 和事实验证 （Fever） 方面，ReAct 通过与简单的维基百科 API 交互，

克服了思维链推理中普遍存在的幻觉和错误传播问题，并生成了类似人类的任务解决轨迹，

这些轨迹比没有推理痕迹的基线更容易解释。在两个交互式决策基准（ALFWorld 和 WebShop）上，

ReAct 的绝对成功率分别以 34% 和 10% 的绝对成功率优于模仿和强化学习方法，同时仅通过一两个上下文示例进行提示。

包含代码的项目站点：此 https URL https://react-lm.github.io/
